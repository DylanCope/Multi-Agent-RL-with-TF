{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Reinforcement Learning with TF-Agents\n",
    "\n",
    "In this notebook we're going to be implementing reinforcement learning (RL) agents to play games against one another. Before reading this it is advised to be familiar with the [TF-Agents](https://github.com/tensorflow/agents) and Deep Q-Learning; [this tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb) will bring you up to speed.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "TF-Agents is a framework for designing and experimenting with RL algorithms. It provides a collection of useful abstractions such as agents, replay buffers, and drivers. However, the code is quite rigidly tied to the single-agent view, which is explained by the *extrinsically motivated* agent in the diagram below.\n",
    "\n",
    "In this view, the environment provides observations and rewards to the agent. Under the assumption that there is only one agent this makes sense, however, when we have many agents in the same space we would like to have agent-specific observations and rewards. In order to rectify this we first need to think of agents as *intrinsically motivated*, which is to say that their rewards are a function of their observations and internal state. Secondly, the agent is only *partially observing* the environment, and the window into the environment is a function of the agent's total state. This total state can include \"physical\" properties of the agent such as position, but it also includes internal state. For example, an agent could have an internal `is_sleeping` parameter that multiplies their observations by zero to simulate a lack of light.\n",
    "\n",
    "## Implementing the IMAgent\n",
    "\n",
    "In order to implement this with TF-Agents we are going to define an `IMAgent` (Intrinsically Motivated Agent) class by overriding the `DqnAgent` class. In the standard TF-Agents DQN pipeline the agent is trained by alternating between data collection and training updates to the Q-Network. Data collection is done with a special `collect_policy` which behaves differently to the main policy for the sake of managing the exploitation-exploration trade-off. Usually, the environment and the agent are separated. The environment generates a `TimeStep` containing the observation and reward information which is then passed to `policy.action`. This produces a `PolicyStep` that contains an action to step the environment. \n",
    "\n",
    "<img src=\"./im_rl_agent.png\" width=\"600px\"/>\n",
    "\n",
    "This provides us with two approaches to our problem. We could make the enviroment aware of which agent it is producing the `TimeStep` for, or we could have each agent ingest an agent-independent time step that is then augmented internally. Here we argue that the latter is a more natural decomposition as it keeps the agent-specific code with the agent class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices:\n",
      " [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] \n",
      "\n",
      "\n",
      "Output Directory: ./outputs/15905145823503500\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import random\n",
    "from time import time\n",
    "from typing import Tuple, List, Callable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import DqnAgent\n",
    "from tf_agents.agents.tf_agent import LossInfo\n",
    "from tf_agents.environments.py_environment import PyEnvironment\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_rnn_network import QRnnNetwork\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.specs import TensorSpec\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories.time_step import TimeStep\n",
    "from tf_agents.trajectories.trajectory import Trajectory\n",
    "\n",
    "print('Physical Devices:\\n', tf.config.list_physical_devices(), '\\n\\n')\n",
    "\n",
    "OUTPUTS_DIR = f'./outputs/{int(10000000 * time())}'\n",
    "print('Output Directory:', OUTPUTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMAgent(DqnAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 env: TFPyEnvironment,\n",
    "                 observation_spec: TensorSpec = None,\n",
    "                 action_spec: TensorSpec = None,\n",
    "                 reward_fn: Callable = lambda time_step: time_step.reward,\n",
    "                 action_fn: Callable = lambda action: action,\n",
    "                 name: str='IMAgent',\n",
    "                 q_network=None,\n",
    "                 # training params\n",
    "                 replay_buffer_max_length: int = 1000,\n",
    "                 learning_rate: float = 1e-5,\n",
    "                 training_batch_size: int = 8,\n",
    "                 training_parallel_calls: int = 3,\n",
    "                 training_prefetch_buffer_size: int = 3,\n",
    "                 training_num_steps: int = 2,\n",
    "                 **dqn_kwargs):\n",
    "\n",
    "        self._env = env\n",
    "        self._reward_fn = reward_fn\n",
    "        self._name = name\n",
    "        self._observation_spec = observation_spec or self._env.observation_spec()\n",
    "        self._action_spec = action_spec or self._env.action_spec()\n",
    "        self._action_fn = action_fn\n",
    "\n",
    "        q_network = q_network or self._build_q_net()\n",
    "\n",
    "        env_ts_spec = self._env.time_step_spec()\n",
    "        time_step_spec = TimeStep(\n",
    "            step_type=env_ts_spec.step_type,\n",
    "            reward=env_ts_spec.reward,\n",
    "            discount=env_ts_spec.discount,\n",
    "            observation=q_network.input_tensor_spec\n",
    "        )\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        super().__init__(time_step_spec,\n",
    "                         self._action_spec,\n",
    "                         q_network,\n",
    "                         optimizer,\n",
    "                         name=name,\n",
    "                         **dqn_kwargs)\n",
    "\n",
    "        self._policy_state = self.policy.get_initial_state(\n",
    "            batch_size=self._env.batch_size)\n",
    "\n",
    "        self._replay_buffer = TFUniformReplayBuffer(\n",
    "            data_spec=self.collect_data_spec,\n",
    "            batch_size=self._env.batch_size,\n",
    "            max_length=replay_buffer_max_length)\n",
    "\n",
    "        dataset = self._replay_buffer.as_dataset(\n",
    "            num_parallel_calls=training_parallel_calls,\n",
    "            sample_batch_size=training_batch_size,\n",
    "            num_steps=training_num_steps\n",
    "        ).prefetch(training_prefetch_buffer_size)\n",
    "\n",
    "        self._training_data_iter = iter(dataset)\n",
    "\n",
    "    def _build_q_net(self):\n",
    "        qrnn = QRnnNetwork(input_tensor_spec=self._observation_spec,\n",
    "                           action_spec=self._action_spec,\n",
    "                           name=f'{self._name}QRNN')\n",
    "\n",
    "        qrnn.create_variables()\n",
    "        qrnn.summary()\n",
    "\n",
    "        return qrnn\n",
    "\n",
    "    def reset_state(self):\n",
    "        self._policy_state = self.policy.get_initial_state(\n",
    "            batch_size=self._env.batch_size)\n",
    "        \n",
    "    def _observation_fn(self, observation: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "            Takes a tensor with specification self._env.observation_spec\n",
    "            and extracts a tensor with specification self._observation_spec.\n",
    "            \n",
    "            For example, consider an agent within an NxN maze environment. \n",
    "            The env could expose the entire NxN integer matrix as an observation\n",
    "            but we would prefer the agent to only see a 3x3 window around their\n",
    "            current location. To do this we can override this method.\n",
    "            \n",
    "            This allows us to have different agents acting in the same environment\n",
    "            with different observations.\n",
    "        \"\"\"\n",
    "        return observation\n",
    "\n",
    "    def _augment_time_step(self, time_step: TimeStep) -> TimeStep:\n",
    "\n",
    "        reward = self._reward_fn(time_step)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        if reward.shape != time_step.reward.shape:\n",
    "            reward = tf.reshape(reward, time_step.reward.shape)\n",
    "            \n",
    "        observation = self._observation_fn(time_step.observation)\n",
    "\n",
    "        return TimeStep(\n",
    "            step_type=time_step.step_type,\n",
    "            reward=reward,\n",
    "            discount=time_step.discount,\n",
    "            observation=observation\n",
    "        )\n",
    "\n",
    "    def _current_time_step(self) -> TimeStep:\n",
    "        time_step = self._env.current_time_step()\n",
    "        time_step = self._augment_time_step(time_step)\n",
    "        return time_step\n",
    "\n",
    "    def _step_environment(self, action) -> TimeStep:\n",
    "        action = self._action_fn(action)\n",
    "        time_step = self._env.step(action)\n",
    "        time_step = self._augment_time_step(time_step)\n",
    "        return time_step\n",
    "\n",
    "    def act(self, collect=False) -> Trajectory:\n",
    "        time_step = self._current_time_step()\n",
    "\n",
    "        if collect:\n",
    "            policy_step = self.collect_policy.action(\n",
    "                time_step, policy_state=self._policy_state)\n",
    "        else:\n",
    "            policy_step = self.policy.action(\n",
    "                time_step, policy_state=self._policy_state)\n",
    "\n",
    "        self._policy_state = policy_step.state\n",
    "        next_time_step = self._step_environment(policy_step.action)\n",
    "        traj = trajectory.from_transition(time_step, policy_step, next_time_step)\n",
    "\n",
    "        if collect:\n",
    "            self._replay_buffer.add_batch(traj)\n",
    "\n",
    "        return traj\n",
    "\n",
    "    def train_iteration(self) -> LossInfo:\n",
    "        experience, buffer_info = next(self._training_data_iter)\n",
    "        return self.train(experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-Tac-Toe Example\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Tic_tac_toe.svg/1200px-Tic_tac_toe.svg.png\" width=\"300px\"/>\n",
    "\n",
    "In order to test this we can utlise the [already-implemented Tic-Tac-Toe environment](https://github.com/tensorflow/agents/blob/master/tf_agents/environments/examples/tic_tac_toe_environment.py) in TF-Agents (At the time of writing this script has not been added to the pip distribution so I have manually copied it across). The environment represents the problem on a 3x3 matrix where a 0 represents an empty slot, a 1 represents a play by player 1, and a 2 represents a play by player 2. However, as TF-Agents is not focused on the multi-agent case, their implementation has the second player act randomly. To change this we will override the step function.\n",
    "\n",
    "The only additional change that we need to make is to the action specification, where we need to provide the value that is being placed (i.e. which player is making the move)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tic_tac_toe_environment import TicTacToeEnvironment\n",
    "from tf_agents.specs import BoundedArraySpec\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "\n",
    "class TicTacToeMultiAgentEnv(TicTacToeEnvironment):\n",
    "    \n",
    "    def action_spec(self):\n",
    "        position_spec = BoundedArraySpec((1,), np.int32, minimum=0, maximum=8)\n",
    "        value_spec = BoundedArraySpec((1,), np.int32, minimum=1, maximum=2)\n",
    "        return {\n",
    "            'position': position_spec,\n",
    "            'value': value_spec\n",
    "        }\n",
    "    \n",
    "    def _step(self, action: np.ndarray):\n",
    "        if self._current_time_step.is_last():\n",
    "            return self._reset()\n",
    "\n",
    "        index_flat = np.array(range(9)) == action['position']\n",
    "        index = index_flat.reshape(self._states.shape) == True\n",
    "        if self._states[index] != 0:\n",
    "            return TimeStep(StepType.LAST, \n",
    "                            TicTacToeEnvironment.REWARD_ILLEGAL_MOVE,\n",
    "                            self._discount, \n",
    "                            self._states)\n",
    "\n",
    "        self._states[index] = action['value']\n",
    "\n",
    "        is_final, reward = self._check_states(self._states)\n",
    "        \n",
    "        if np.all(self._states == 0):\n",
    "            step_type = StepType.FIRST\n",
    "        elif is_final:\n",
    "            step_type = StepType.LAST\n",
    "        else:\n",
    "            step_type = StepType.MID\n",
    "\n",
    "        return TimeStep(step_type, reward, self._discount, self._states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tic_tac_toe(state):\n",
    "    table_str = '''\n",
    "    {} | {} | {}\n",
    "    - + - + -\n",
    "    {} | {} | {}\n",
    "    - + - + -\n",
    "    {} | {} | {}\n",
    "    '''.format(*tuple(state.flatten()))\n",
    "    table_str = table_str.replace('0', ' ')\n",
    "    table_str = table_str.replace('1', 'X')\n",
    "    table_str = table_str.replace('2', 'O')\n",
    "    print(table_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.0 Board:\n",
      "\n",
      "      |   |  \n",
      "    - + - + -\n",
      "      |   |  \n",
      "    - + - + -\n",
      "      |   |  \n",
      "    \n",
      "Player: 1 Action: 2 Reward: 0.0 Board:\n",
      "\n",
      "      |   | X\n",
      "    - + - + -\n",
      "      |   |  \n",
      "    - + - + -\n",
      "      |   |  \n",
      "    \n",
      "Player: 2 Action: 1 Reward: 0.0 Board:\n",
      "\n",
      "      | O | X\n",
      "    - + - + -\n",
      "      |   |  \n",
      "    - + - + -\n",
      "      |   |  \n",
      "    \n",
      "Player: 1 Action: 4 Reward: 0.0 Board:\n",
      "\n",
      "      | O | X\n",
      "    - + - + -\n",
      "      | X |  \n",
      "    - + - + -\n",
      "      |   |  \n",
      "    \n",
      "Player: 2 Action: 1 Reward: -0.001 Board:\n",
      "\n",
      "      | O | X\n",
      "    - + - + -\n",
      "      | X |  \n",
      "    - + - + -\n",
      "      |   |  \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "tic_tac_toe_env = TicTacToeMultiAgentEnv()\n",
    "\n",
    "ts = tic_tac_toe_env.reset()\n",
    "print('Reward:', ts.reward, 'Board:')\n",
    "print_tic_tac_toe(ts.observation)\n",
    "\n",
    "random.seed(1)\n",
    "player = 1\n",
    "while not ts.is_last():\n",
    "    action = {\n",
    "        'position': np.asarray(random.randint(0, 8)),\n",
    "        'value': player\n",
    "    }\n",
    "    ts = tic_tac_toe_env.step(action)\n",
    "    print('Player:', player, 'Action:', action['position'],\n",
    "          'Reward:', ts.reward, 'Board:')\n",
    "    print_tic_tac_toe(ts.observation)\n",
    "    player = 1 + player % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Player1QRNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  3790      \n",
      "_________________________________________________________________\n",
      "dynamic_unroll_3 (DynamicUnr multiple                  12960     \n",
      "_________________________________________________________________\n",
      "Player1QRNN/dense (Dense)    multiple                  3075      \n",
      "_________________________________________________________________\n",
      "Player1QRNN/dense (Dense)    multiple                  3040      \n",
      "_________________________________________________________________\n",
      "num_action_project/dense (De multiple                  369       \n",
      "=================================================================\n",
      "Total params: 23,234\n",
      "Trainable params: 23,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'rank'\n  In call to configurable 'DqnAgent' (<function DqnAgent.__init__ at 0x00000240FC493E18>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-f71a51eddc48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0maction_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtic_tac_toe_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'position'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0maction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mttt_action_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Player1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-6f6da4a896bc>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env, observation_spec, action_spec, reward_fn, action_fn, name, q_network, replay_buffer_max_length, learning_rate, training_batch_size, training_parallel_calls, training_prefetch_buffer_size, training_num_steps, **dqn_kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m                          \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                          \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                          **dqn_kwargs)\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         self._policy_state = self.policy.get_initial_state(\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\xai-it\\lib\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m         \u001b[0mscope_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" in scope '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m         \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\xai-it\\lib\\site-packages\\gin\\utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mExceptionProxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\xai-it\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\xai-it\\lib\\site-packages\\gin\\config.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1009\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\xai-it\\lib\\site-packages\\tf_agents\\agents\\dqn\\dqn_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, time_step_spec, action_spec, q_network, optimizer, observation_and_action_constraint_splitter, epsilon_greedy, n_step_update, boltzmann_temperature, emit_log_probability, target_q_network, target_update_tau, target_update_period, td_errors_loss_fn, gamma, reward_scale_factor, gradient_clipping, debug_summaries, summarize_grads_and_vars, train_step_counter, name)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_action_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepsilon_greedy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mboltzmann_temperature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\xai-it\\lib\\site-packages\\tf_agents\\agents\\dqn\\dqn_agent.py\u001b[0m in \u001b[0;36m_check_action_spec\u001b[1;34m(self, action_spec)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# TODO(oars): Get DQN working with more than one dim in the actions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_action_spec\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mflat_action_spec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Only one dimensional actions are supported now.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'\n  In call to configurable 'DqnAgent' (<function DqnAgent.__init__ at 0x00000240FC493E18>)"
     ]
    }
   ],
   "source": [
    "def ttt_action_fn(player, action):\n",
    "    return {'position': action, 'value': player}\n",
    "\n",
    "tf_ttt_env = TFPyEnvironment(tic_tac_toe_env)\n",
    "\n",
    "player_1 = IMAgent(\n",
    "    tf_ttt_env,\n",
    "    action_spec = tic_tac_toe_env.action_spec()['position'],\n",
    "    action_fn = partial(ttt_action_fn, 1),\n",
    "    name='Player1'\n",
    ")\n",
    "\n",
    "player_2 = IMAgent(\n",
    "    tf_ttt_env,\n",
    "    action_spec = tic_tac_toe_env.action_spec()['position'],\n",
    "    action_fn = partial(ttt_action_fn, 2),\n",
    "    reward_fn = lambda ts: -1.0 if ts.reward == 1.0 else ts.reward,\n",
    "    name='Player2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
